{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GQ4_okl1InlREAUhnr6xdTdYvF7msgeD","timestamp":1683481820128}],"gpuType":"A100","collapsed_sections":["JV3kUeMA9YWU"],"machine_shape":"hm","mount_file_id":"1KfHAPyEGfhqBsQwDI01TI0rW5cd3Vz_a","authorship_tag":"ABX9TyMHrsZRsHvAtxa1KAkYp+7X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"f8e25f16fe0242c1ae518bb6a115717f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d6021958ad34dd0a465be731d720047","IPY_MODEL_808997f8ac454fafaa44184aafebcd14","IPY_MODEL_b8788147e2794a3ba7c9de26adf20592"],"layout":"IPY_MODEL_d293cb71a1ca4de4a4ad28a14d024920"}},"2d6021958ad34dd0a465be731d720047":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_092add6213a6474bb2592a52ccb0030a","placeholder":"​","style":"IPY_MODEL_3a5a764a77bd4c298fef1dd727f6f09b","value":"Loading checkpoint shards: 100%"}},"808997f8ac454fafaa44184aafebcd14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0994250569bc409f8180bf76f8b2caa0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e2a1f3e35fd4aa4a2fc1fc8ac1ed868","value":2}},"b8788147e2794a3ba7c9de26adf20592":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ba13156352741ba8ba33b91edb74c8b","placeholder":"​","style":"IPY_MODEL_226a167440644f34941ff49ee62db4e9","value":" 2/2 [03:02&lt;00:00, 82.44s/it]"}},"d293cb71a1ca4de4a4ad28a14d024920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"092add6213a6474bb2592a52ccb0030a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5a764a77bd4c298fef1dd727f6f09b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0994250569bc409f8180bf76f8b2caa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e2a1f3e35fd4aa4a2fc1fc8ac1ed868":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ba13156352741ba8ba33b91edb74c8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226a167440644f34941ff49ee62db4e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Copyright 2022 MosaicML LLM Foundry authors.\n","SPDX-License-Identifier: Apache-2.0.\n","Re-Authored for Jupyter: Shawn \"EclipsePhage\" Hinzey.\n","V. 7May23_522"],"metadata":{"id":"DkPbDZzih1-V"}},{"cell_type":"markdown","source":["\n","Instructions:\n","*   Do not use GPU runtime unless you're prepared to run.\n","*   Decide if you're 'greedy' or not.\n","*   Decide if you would like to chunk.\n","*   List item\n","\n","\n","\n"],"metadata":{"id":"gLkJeiPJgkvR"}},{"cell_type":"markdown","source":["#Installations"],"metadata":{"id":"hbkJxBN15qhW"}},{"cell_type":"code","source":["#For potentially mounting model \"locally\"\n","!pip install git-lfs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqFwKBhhAlSJ","executionInfo":{"status":"ok","timestamp":1683688963580,"user_tz":360,"elapsed":6569,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"d86f6d39-4b31-489c-ec04-fcac276b5495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: git-lfs in /usr/local/lib/python3.10/dist-packages (1.6)\n"]}]},{"cell_type":"code","source":["## Optional, if Colab struggles with loading via HF API.  Uncomment if needed. Adjust name_or_path variable to point to file.\n","## Recommend installing to a Google Drive and mounting it.  Change name_or_path to the drive when complete.\n","!git lfs install\n","!git lfs clone -v https://huggingface.co/mosaicml/mpt-7b-storywriter /content/drive/MyDrive/AIML_models/mpt-7b-storywriter"],"metadata":{"id":"Mhm9v_3L8AhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqqfKelH1x-3","executionInfo":{"status":"ok","timestamp":1683692234037,"user_tz":360,"elapsed":12584,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"840645a1-c98c-434f-a26e-3421869b818c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["#Print current GPU configuration\n","!nvidia-smi\n","print(\"\\nVerify your runtime is GPU-based w/High-Ram. Set and re-run notebook if not.\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AdDRbmSyf4G","executionInfo":{"status":"ok","timestamp":1683692237667,"user_tz":360,"elapsed":668,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"59d43afa-5800-40e1-af89-cfd2b9fc62cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed May 10 04:17:12 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","\n","Verify your runtime is GPU-based w/High-Ram. Set and re-run notebook if not.\n","\n"]}]},{"cell_type":"code","source":["!pip install einops #install einops"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymv5RwAF38mc","executionInfo":{"status":"ok","timestamp":1683692247743,"user_tz":360,"elapsed":5132,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"baac61b0-d10d-4210-855f-ee57f6e5ae09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m167.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.1\n"]}]},{"cell_type":"code","source":["#THIS INSTALL TAKES FOREVER\n","#!pip3 install flash-attn==1.0.3.post0  #install flash-attn (optional depending on args)"],"metadata":{"id":"qDU-dzTUP5W1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Install flash-attn prior to triton if needed (optional depending on args)\n","#!pip install triton==2.0.0.dev20221202  #install triton implementation of flash-attn (optional depending on args)"],"metadata":{"id":"ixEyP-iTQHBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#set huggingface token (optional if required by the model)\n","import os\n","\n","huggingface_token = input(\"Please enter your Hugging Face token: \")\n","os.environ['HUGGING_FACE_HUB_TOKEN'] = huggingface_token\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_zbg64EqX7d","executionInfo":{"status":"ok","timestamp":1683692283620,"user_tz":360,"elapsed":4499,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"dada69d1-ae20-4340-d029-453d666da6b9"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter your Hugging Face token: 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\n"]}]},{"cell_type":"markdown","source":["#Run model (one-shot) via hf_generate.py (not preffered). Do not run unless you're sure. Uncomment line to run."],"metadata":{"id":"JV3kUeMA9YWU"}},{"cell_type":"code","source":["#Run the model in one-shot w/o being celled in colab.  Note: you must have hf_generate.py uploaded to session storage.\n","#!python hf_generate.py --name_or_path mosaicml/mpt-7b-storywriter --prompts \"Once upon a time\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpPUrGnV3d_1","executionInfo":{"status":"ok","timestamp":1683479172423,"user_tz":360,"elapsed":394563,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"614bc6ee-5908-47c4-a31b-ec5730c60611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading HF Config...\n","Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n","Loading HF model to device=cuda:0 and dtype=torch.bfloat16...\n","Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n","/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-storywriter/26f3be4e1d8bfe3d4313408401582f960f79ade7/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n","  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n","Loading checkpoint shards: 100% 2/2 [01:05<00:00, 32.77s/it]\n","n_params=6649286656\n","\n","Loading HF tokenizer...\n","/content/hf_generate.py:169: UserWarning: pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\n","  warnings.warn(\n","\n","Generate kwargs:\n","{'max_new_tokens': 100, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'use_cache': True, 'do_sample': True, 'eos_token_id': 0, 'pad_token_id': 0}\n","\n","Tokenizing prompts...\n","NOT using autocast...\n","Warming up...\n","Generating responses...\n","2023-05-07 17:06:04.199920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","####################################################################################################\n","\u001b[92mOnce upon a time\u001b[0m in the south of Spain a young Spaniard was very brave and very strong.\n","\n","His name was Miguel de Cervantes. His father made shoes and he also worked for the landlord who owned his farm, even though he didn't like having to do it.\n","\n","Cervantes always went to the same school in his town. Everyone in the town knew him. But he was especially fond of a teacher called Juan de Borgia because Borgia was a very kind teacher and a very\n","####################################################################################################\n","bs=1, input_tokens=array([4]), output_tokens=array([100])\n","total_input_tokens=4, total_output_tokens=100\n","encode_latency=10.08ms, gen_latency=5816.36ms, decode_latency=8636.10ms, total_latency=14462.54ms\n","latency_per_output_token=144.63ms/tok\n","output_tok_per_sec=6.91tok/sec\n"]}]},{"cell_type":"markdown","source":["# Run model (celled/Pre-loaded). Preferred for testing."],"metadata":{"id":"i_THOPEOegTi"}},{"cell_type":"markdown","source":["Import packages and define helper functions"],"metadata":{"id":"FuqiWvPSfaNQ"}},{"cell_type":"code","source":["import random\n","import time\n","import warnings\n","from argparse import Namespace\n","from contextlib import nullcontext\n","\n","import torch\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n","\n","def get_dtype(dtype):\n","    if dtype == 'fp32':\n","        return torch.float32\n","    elif dtype == 'fp16':\n","        return torch.float16\n","    elif dtype == 'bf16':\n","        return torch.bfloat16\n","    else:\n","        raise NotImplementedError(\n","            f'dtype {dtype} is not supported. '\n","            f'We only support fp32, fp16, and bf16 currently')\n","\n","def str2bool(v):\n","    if isinstance(v, bool):\n","        return v\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise ArgumentTypeError('Boolean value expected.')\n","\n","def str_or_bool(v):\n","    if isinstance(v, bool):\n","        return v\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        return v\n","\n","def maybe_synchronize():\n","    if torch.cuda.is_available():\n","        torch.cuda.synchronize()\n"],"metadata":{"id":"8Zb3d0vmefGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Check CUDA RAM Allocation**"],"metadata":{"id":"UJO97mfao5LR"}},{"cell_type":"code","source":["import os\n","\n","alloc_conf = os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", None)\n","\n","if alloc_conf is not None:\n","    configs = alloc_conf.split(',')\n","    max_split_size = [conf for conf in configs if conf.startswith('max_split_size_mb')]\n","\n","    if max_split_size:\n","        print(max_split_size[0])\n","    else:\n","        print(\"max_split_size_mb is not set.\")\n","else:\n","    print(\"PYTORCH_CUDA_ALLOC_CONF is not set.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdCp_IUEo2Tm","executionInfo":{"status":"ok","timestamp":1683520453845,"user_tz":360,"elapsed":400,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"ba8bb5fe-e9d5-49d9-b6f5-23d64742a039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PYTORCH_CUDA_ALLOC_CONF is not set.\n"]}]},{"cell_type":"markdown","source":["**Set CUDA RAM Allocation**"],"metadata":{"id":"FkE3gctlhLJ0"}},{"cell_type":"code","source":["## Checks and sets CUDA RAM allocation split size\n","import os\n","\n","#this sets max split size to remedy OOM issues.  Default is not set.  Try 3000 or less.\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb=300\"  #On T4-15GB\n","\n","## (Un)comment if statement to set/unset the RAM environment var an re-run cell.\n","if \"PYTORCH_CUDA_ALLOC_CONF\" in os.environ:\n","    del os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n","\n","alloc_conf = os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", None)\n","\n","if alloc_conf is not None:\n","    configs = alloc_conf.split(',')\n","    max_split_size = [conf for conf in configs if conf.startswith('max_split_size_mb')]\n","\n","    if max_split_size:\n","        print(max_split_size[0])\n","    else:\n","        print(\"max_split_size_mb is not set.\")\n","else:\n","    print(\"PYTORCH_CUDA_ALLOC_CONF is not set.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abp__tI5ePof","executionInfo":{"status":"ok","timestamp":1683519458529,"user_tz":360,"elapsed":268,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"5a97970d-af4b-40d7-a023-bbb5ce332bf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PYTORCH_CUDA_ALLOC_CONF is not set.\n"]}]},{"cell_type":"markdown","source":["#**Set up your PROMPTS + arguments*****"],"metadata":{"id":"ZFonavqAfh65"}},{"cell_type":"markdown","source":["These arguments control various aspects of text generation and model configuration. Here's a description of each argument:\n","\n","1. `max_new_tokens`: The maximum number of tokens to generate in the output. This limits the length of the generated text.\n","2. `max_seq_len`: The maximum sequence length the model accepts. If not provided, it will use the model's default value.\n","3. `temperature`: Controls the randomness of the output. Higher values (e.g., 1.0) result in more diverse and creative text, while lower values (e.g., 0.1) make the output more focused and deterministic.\n","4. `top_k`: The number of top tokens to consider for each step of the text generation. This can prevent the model from sampling low-probability words.\n","5. `top_p`: The cumulative probability threshold for token sampling. This is an alternative to `top_k` and can be used to dynamically adapt the number of tokens considered at each generation step.\n","6. `do_sample`: Whether to sample the output tokens or use a deterministic approach (e.g., selecting the highest probability token).\n","7. `use_cache`: Whether to use the model's cache to speed up decoding.\n","8. `eos_token_id`: The ID of the end-of-sentence token. If not provided, it will use the tokenizer's default value.\n","9. `pad_token_id`: The ID of the padding token. If not provided, it will use the tokenizer's default value.\n","10. `model_dtype`: The data type used by the model. If not provided, it will use the default data type of the model.\n","11. `autocast_dtype`: The data type for autocasting. If provided, the model will use autocasting with the specified data type.\n","12. `warmup`: Whether to perform a warmup generation before starting the actual generation. This can help reduce latency for the first generation.\n","13. `trust_remote_code`: Whether to trust remote code execution. This is relevant when using Hugging Face Hub models that may contain custom code.\n","14. `use_auth_token`: An authentication token for accessing private models on the Hugging Face Hub.\n","15. `revision`: The specific version of the model to use from the Hugging Face Hub.\n","16. `device`: The device to run the model on, such as 'cpu' or 'cuda'. If not provided, it will use the available device.\n","17. `attn_impl`: The attention implementation to use. If not provided, it will use the default implementation.\n","18. `seed`: The random seed to set for reproducibility.\n","\n","These arguments allow you to control the behavior of the model, the text generation process, and other aspects of the script."],"metadata":{"id":"UaFyhSpe5FIf"}},{"cell_type":"markdown","source":["#ARGS/DEFINES"],"metadata":{"id":"MdCP6bfanYc9"}},{"cell_type":"code","source":["# Note: depending on which args you change, you may or may not need to reload certain cells below!!!!!\n","IS_GREEDY = False\n","IS_CHUNKY = False\n","\n","args = Namespace(\n","    #name_or_path=\"mosaicml/mpt-7b-storywriter\",\n","    name_or_path=\"/content/drive/MyDrive/AIML_models/mpt-7b-storywriter\",  #local files\n","\n","    max_new_tokens=200,  #default=50\n","    max_seq_len=83968,  #default None\n","    temperature=0.7,  # 0-1.0 default=0.8\n","    top_k=0,  #default WAS 50, but reset to 0\n","    top_p=1.0, # 0-1; default 1\n","    do_sample=True,\n","    use_cache=True,\n","    eos_token_id=None,\n","    pad_token_id=None,\n","    model_dtype=None,    #default None.  'fp16' should conserve RAM.\n","    autocast_dtype=None,  #default None (bf16/fp16 should conserve RAM)\n","    warmup=False,  #False may conserve RAM.  default True.\n","\n","    trust_remote_code=True,\n","    use_auth_token=None,\n","    revision=None,\n","\n","    device=None,\n","    attn_impl=None,  #could be 'triton' (faster) or 'flash' (less RAM) etc. (must reload HF Config)\n","                     #defaults is None, which sets to 'torch'.  Note: triton/flash will not work with Torch2.0\n","      \n","    no_repeat_ngram_size=0,  #Default is 6\n","    seed=42,\n","    prompts= [\"In times long past, upon a fateful day when my devoted hound suffered a grievous blow to her head, a most peculiar transformation occurred. Her very essence seemed to be overtaken by the spirit of a bygone outlaw from the untamed western territories. This otherworldly desperado implored me to join forces and embark on an audacious enterprise— to plunder the storied Dragon Train of its abundant treasures. Enticed by the promise of immeasurable wealth, I found myself torn between worldly desires and the innate moral compass guiding my soul. In my moment of inner struggle, I sought solace in the sagacity of my forebears, who had navigated life's tribulations with unwavering adherence to the virtues of truth, honor, and compassion. Resolute, I chose to pursue the path of redemption, striving to mend the once unsullied bond with my cherished canine companion, rather than yielding to the transitory allure of material gain.\"]\n",")\n"],"metadata":{"id":"krFEV35WfgGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Load HF configs, tokenizer, device, and model (~5min+ total). "],"metadata":{"id":"Z89qjmsZpRRk"}},{"cell_type":"code","source":["# Load the HF Config\n","print(f'Loading HF Config...')\n","from_pretrained_kwargs = {\n","    'use_auth_token': args.use_auth_token,\n","    'trust_remote_code': args.trust_remote_code,\n","    'revision': args.revision,\n","}\n","try:\n","    config = AutoConfig.from_pretrained(args.name_or_path,\n","                                        **from_pretrained_kwargs)\n","    if args.attn_impl is not None and hasattr(config, 'attn_config'):\n","        config.attn_config['attn_impl'] = args.attn_impl\n","    if args.max_seq_len is not None and hasattr(config, 'max_seq_len'):\n","        config.max_seq_len = args.max_seq_len\n","\n","except Exception as e:\n","    raise RuntimeError(\n","        'If you are having auth problems, try logging in via `huggingface-cli login` '\n","        'or by setting the environment variable `export HUGGING_FACE_HUB_TOKEN=... '\n","        'using your access token from https://huggingface.co/settings/tokens.'\n","    ) from e"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oM4TguBIhkyX","executionInfo":{"status":"ok","timestamp":1683693190854,"user_tz":360,"elapsed":517,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"f068f017-e536-43c8-ecd5-16a75284e22f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"]},{"output_type":"stream","name":"stdout","text":["Loading HF Config...\n"]}]},{"cell_type":"code","source":["# Set device and model_dtype\n","if args.device is not None:\n","    device = args.device\n","else:\n","    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","if args.model_dtype is not None:\n","    model_dtype = get_dtype(args.model_dtype)\n","else:\n","    model_dtype = config.torch_dtype or torch.float32"],"metadata":{"id":"xvJc6ZPvhufL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Be mindful of connecting to HF and downloading large models via this api call.  This will cost you compute time.  It's smarter to mount to a Google Drive ONCE and save this time.\n","As the below cell runs, you will see the System RAM increase as the model is being loaded.  Be sure to leave enough overhead RAM.  If you find yourself in the red, change the runtime type to High-Ram and build again.  When the load is complete, systemRAM will drop and GPU will increase. T4, A100, and V100 all have different capabilities.  You will have to experiment."],"metadata":{"id":"hj-g3B0_BcuO"}},{"cell_type":"code","source":["# Load HF Model (this takes time 5m+)\n","print(f'Loading HF model to device={device} and dtype={model_dtype}...')\n","try:\n","    model = AutoModelForCausalLM.from_pretrained(args.name_or_path,\n","                                                  config=config,\n","                                                  torch_dtype=model_dtype,\n","                                                  **from_pretrained_kwargs)\n","    model.to(device)\n","    model.eval()\n","    print(f'n_params={sum(p.numel() for p in model.parameters())}')\n","except Exception as e:\n","    raise RuntimeError(\n","        'If you are having auth problems, try logging in via `huggingface-cli login` '\n","        'or by setting the environment variable `export HUGGING_FACE_HUB_TOKEN=... '\n","        'using your access token from https://huggingface.co/settings/tokens.'\n","    ) from e"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["f8e25f16fe0242c1ae518bb6a115717f","2d6021958ad34dd0a465be731d720047","808997f8ac454fafaa44184aafebcd14","b8788147e2794a3ba7c9de26adf20592","d293cb71a1ca4de4a4ad28a14d024920","092add6213a6474bb2592a52ccb0030a","3a5a764a77bd4c298fef1dd727f6f09b","0994250569bc409f8180bf76f8b2caa0","9e2a1f3e35fd4aa4a2fc1fc8ac1ed868","0ba13156352741ba8ba33b91edb74c8b","226a167440644f34941ff49ee62db4e9"]},"id":"FA38iAOph09n","executionInfo":{"status":"ok","timestamp":1683692918776,"user_tz":360,"elapsed":499047,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"b29b0a59-5837-4c7d-d511-43c504045174"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"]},{"output_type":"stream","name":"stdout","text":["Loading HF model to device=cuda:0 and dtype=torch.bfloat16...\n"]},{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/transformers_modules/mpt-7b-storywriter/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n","  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e25f16fe0242c1ae518bb6a115717f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["n_params=6649286656\n"]}]},{"cell_type":"code","source":["# Load the HF tokenizer\n","from transformers import AutoTokenizer\n","\n","print('\\nLoading HF tokenizer...')\n","#optional gpt-neox-20b tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n","\n","#default tokenizer\n","#tokenizer = AutoTokenizer.from_pretrained(args.name_or_path,\n","#                                          **from_pretrained_kwargs)\n","if tokenizer.pad_token_id is None:\n","    warnings.warn(\n","        'pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.'\n","    )\n","    tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'left'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0sgBfARjCRr","executionInfo":{"status":"ok","timestamp":1683696118281,"user_tz":360,"elapsed":3,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"6f56b2ed-b871-4a8d-81a4-47d18a04158f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading HF tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-90-989863a080fe>:12: UserWarning: pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Run to print output to output.txt"],"metadata":{"id":"iWTSkG3_6NJe"}},{"cell_type":"code","source":["##Run this cell to print to an output.txt\n","#import sys\n","\n","## Save the original stdout\n","#original_stdout = sys.stdout\n","\n","## Open the output file in append mode\n","#output_file = open(\"output.txt\", \"a\")\n","\n","#sys.stdout = output_file"],"metadata":{"id":"1LB0HcgFkfXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","# May need to use this cell if autocast isn't working (you will get an error at generation)\n","# Check if a GPU is available and set the device accordingly\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"],"metadata":{"id":"pA0-XgdCNOT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Re-run this cell whenever changing PROMPTS, cast-type, or KWARGS!!"],"metadata":{"id":"oRqva94-owE4"}},{"cell_type":"code","source":["#@title Default title text\n","# Generating kwargs, Tokenizing!, and casting/contextualizing\n","\n","generate_kwargs = {\n","    'max_new_tokens': args.max_new_tokens,\n","    'temperature': args.temperature,\n","    'top_p': args.top_p,\n","    'top_k': args.top_k,\n","    'use_cache': args.use_cache,\n","    'do_sample': args.do_sample,\n","    'eos_token_id': args.eos_token_id or tokenizer.eos_token_id,\n","    'pad_token_id': args.pad_token_id or tokenizer.pad_token_id,\n","}\n","print(f'\\nGenerate kwargs:\\n{generate_kwargs}')\n","\n","print(f'\\nTokenizing prompts...')\n","maybe_synchronize()\n","encode_start = time.time()\n","encoded_inp = tokenizer(args.prompts, return_tensors='pt', padding=True)\n","for key, value in encoded_inp.items():\n","    encoded_inp[key] = value.to(device)\n","maybe_synchronize()\n","encode_end = time.time()\n","input_tokens = torch.sum(encoded_inp['input_ids'] != tokenizer.pad_token_id,\n","                          axis=1).numpy(force=True)  # type: ignore\n","\n","# Autocast\n","if args.autocast_dtype is not None:\n","    autocast_dtype = get_dtype(args.autocast_dtype)\n","    autocast_context = torch.autocast(device, autocast_dtype)\n","    print(f'Using autocast with dtype={autocast_dtype}...')\n","else:\n","    autocast_context = nullcontext()\n","    print('NOT using autocast...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ep3e6NdSYuGu","executionInfo":{"status":"ok","timestamp":1683696177926,"user_tz":360,"elapsed":331,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"aca4a612-868b-41c9-c6ea-2afa29000161"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generate kwargs:\n","{'max_new_tokens': 200, 'temperature': 0.7, 'top_p': 1.0, 'top_k': 0, 'use_cache': True, 'do_sample': True, 'eos_token_id': 0, 'pad_token_id': 0}\n","\n","Tokenizing prompts...\n","NOT using autocast...\n"]}]},{"cell_type":"markdown","source":["# **GENERATION**"],"metadata":{"id":"gChbMh2UY0jk"}},{"cell_type":"markdown","source":["Take note of the cell line the below cell stops execution on.  This will indicated how well the GPU is doing considering VRAM allocation."],"metadata":{"id":"YAIzXvhPn1e_"}},{"cell_type":"code","source":["\n","\n","# Generate function with correct context managers  \n","def _generate(encoded_inp):\n","    with torch.no_grad():\n","        with autocast_context:\n","            return model.generate(\n","                input_ids=encoded_inp['input_ids'],\n","                attention_mask=encoded_inp['attention_mask'],\n","                **generate_kwargs,\n","            )\n","\n","# Warmup\n","if args.warmup:\n","    print('Warming up...')\n","    _ = _generate(encoded_inp)\n","\n","# Seed randomness\n","print('Seeding...')\n","random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","\n","# Run HF generate\n","print('Generating responses...')\n","maybe_synchronize()\n","gen_start = time.time()\n","encoded_gen = _generate(encoded_inp)  #comment out if using greedy generation\n","maybe_synchronize()\n","gen_end = time.time()\n","\n","print('Decoding responses...')\n","decode_start = time.time()\n","decoded_gen = tokenizer.batch_decode(encoded_gen, skip_special_tokens=True)  #comment out if using greedy generation\n","maybe_synchronize()\n","decode_end = time.time()\n","gen_tokens = torch.sum(encoded_gen != tokenizer.pad_token_id,   #comment out if using greedy generation\n","                        axis=1).numpy(force=True)  # type: ignore\n","\n","# Print generations  ##remove/comment out this section if using greedy generation\n","delimiter = '#' * 100\n","for prompt, gen in zip(args.prompts, decoded_gen):\n","    continuation = gen[len(prompt):]\n","    print(delimiter)\n","    print('\\033[92m' + prompt + '\\033[0m' + continuation)\n","print(delimiter)\n","\n","# Print timing info\n","bs = len(args.prompts)\n","output_tokens = gen_tokens - input_tokens\n","total_input_tokens = input_tokens.sum()\n","total_output_tokens = output_tokens.sum()\n","encode_latency = 1000 * (encode_end - encode_start)\n","gen_latency = 1000 * (gen_end - gen_start)\n","decode_latency = 1000 * (decode_end - decode_start)\n","total_latency = encode_latency + gen_latency + decode_latency\n","\n","latency_per_output_token = total_latency / total_output_tokens\n","output_tok_per_sec = 1000 / latency_per_output_token\n","print(f'{bs=}, {input_tokens=}, {output_tokens=}')\n","print(f'{total_input_tokens=}, {total_output_tokens=}')\n","print(\n","    f'{encode_latency=:.2f}ms, {gen_latency=:.2f}ms, {decode_latency=:.2f}ms, {total_latency=:.2f}ms'\n",")\n","print(f'{latency_per_output_token=:.2f}ms/tok')\n","print(f'{output_tok_per_sec=:.2f}tok/sec')\n","\n","#Run complete"],"metadata":{"id":"fAbfZXONfqLH","cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683696188061,"user_tz":360,"elapsed":6156,"user":{"displayName":"Shawn Hinzey","userId":"06547730696055549564"}},"outputId":"33520746-8c66-4ab1-c970-62e70ede1b5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Seeding...\n","Generating responses...\n","Decoding responses...\n","####################################################################################################\n","\u001b[92mIn times long past, upon a fateful day when my devoted hound suffered a grievous blow to her head, a most peculiar transformation occurred. Her very essence seemed to be overtaken by the spirit of a bygone outlaw from the untamed western territories. This otherworldly desperado implored me to join forces and embark on an audacious enterprise— to plunder the storied Dragon Train of its abundant treasures. Enticed by the promise of immeasurable wealth, I found myself torn between worldly desires and the innate moral compass guiding my soul. In my moment of inner struggle, I sought solace in the sagacity of my forebears, who had navigated life's tribulations with unwavering adherence to the virtues of truth, honor, and compassion. Resolute, I chose to pursue the path of redemption, striving to mend the once unsullied bond with my cherished canine companion, rather than yielding to the transitory allure of material gain.In times long past, upon a fateful day when my devoted hound suffered a grievous blow to her head, a most peculiar transformation occurred. Her very essence seemed to be overtaken by the spirit of a bygone outlaw from the untamed western territories. This otherworldly desperado implored me to join forces and embark on an audacious enterprise— to plunder the storied Dragon Train of its abundant treasures. Enticed by the promise of immeasurable wealth, I found myself torn between worldly desires and the innate moral compass guiding my soul. In my moment of inner struggle, I sought solace in the sagacity of my forebears, who had navigated life's tribulations with unwavering adherence to the virtues of truth, honor, and compassion. Resolute, I chose to pursue the path of redemption, striving to mend the once unsullied bond with my cherished canine companion, rather than yielding to the transitory allure of material gain. However, the dog's fateful injury and my failure to help her resulted in a profound fracture in the bond between us. Unable to bear this permanent rift, I resolved to rid myself of my beloved hound, hoping to find solace in the arms of a new lover.\n","\n","\"Upon learning of my decision, however, the outlaw spirit of the dog's father, the mighty warrior knight Sir Pug, arose from the abyss of time and intervened. It was then that Sir Pug, in a vision of great clarity, revealed a secret path to the Dragon Train. He cautioned me to be wary of this bewitching treasure trove, as it had once been the object of a sinister plot by the demon lord of the frozen wastes, who would stop at nothing to reclaim his stolen treasure. The noble knight added that if I were to seek the Dragon Train, it would be with a new companion— a female companion who would be much more than a mere\u001b[0m dog.\n","\n","\"With the guidance of my stalwart forebears, I set forth on a perilous journey to the faraway lands of Avalon, where I was met by a most unusual female canine. At first, I mistook her for a mere dog, but she proved to be much more than a mere companion. She was skilled in the ways of the earth and water, and she resided within a lair that was a temple to the ancient gods of the past.\"\n","\n","\"A temple to the ancient gods? In Avalon?\"\n","\n","\"Such is the way of the world. Time is a malleable force, and as such, it is constantly shifting and changing. Sir Pug had foreseen this shift in his vision, and thus he devised a plan to thwart the demon lord's return to the frozen tundra.\n","\n","\"The demon lord's treacherous plan was to unleash the beast of the serpent's head, a\n","####################################################################################################\n","bs=1, input_tokens=array([602]), output_tokens=array([200])\n","total_input_tokens=602, total_output_tokens=200\n","encode_latency=3.87ms, gen_latency=6032.80ms, decode_latency=1.65ms, total_latency=6038.32ms\n","latency_per_output_token=30.19ms/tok\n","output_tok_per_sec=33.12tok/sec\n"]}]},{"cell_type":"code","source":["## Restore the original stdout\n","#sys.stdout = original_stdout\n","## Close the output file\n","#output_file.close()\n","print(\"\\n\\nProgram Complete!  Be sure to disconnect and delete runtime.\\n\")"],"metadata":{"id":"zUbr1dHjrc74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(decoded_gen)"],"metadata":{"id":"Lp8uJssIgzPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #Set prompt to the prompt + output\n","args.prompts = [prompt + gen for prompt, gen in zip(args.prompts, decoded_gen)]\n","#args.prompts = decoded_gen\n","\n"],"metadata":{"id":"RkhWotE3bRCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoded_gen_str = \"\\n\".join(decoded_gen)\n","\n","with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n","    file.write(decoded_gen_str)\n","    file.write(\"\\n\")\n","\n","#with open(\"output.txt\", \"a\") as file:\n","#    file.write(decoded_gen)"],"metadata":{"id":"df-5u8bshqF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Housekeeping"],"metadata":{"id":"rEJyIKlIY-AM"}},{"cell_type":"code","source":["# GPU Memory Housekeeping\n","# Delete encoded_inp tensor after generating responses\n","del encoded_inp\n","torch.cuda.empty_cache()\n","\n","# Delete encoded_gen tensor after printing generations\n","del encoded_gen\n","torch.cuda.empty_cache()"],"metadata":{"id":"5OeAfoCgAA1o"},"execution_count":null,"outputs":[]}]}